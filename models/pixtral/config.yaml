datasets:
  - path: models/pixtral/dataset/synthetic_financial_pix_dataset.jsonl
    type: alpaca_chat

model:
  name: mistral-7B-instruct-v0.1
  path: models/pixtral/pixtral-7B-instruct-v0.1

training:
  output_dir: models/pixtral/finetune
  num_train_epochs: 6
  per_device_train_batch_size: 16
  per_device_eval_batch_size: 8
  gradient_accumulation_steps: 1
  evaluation_strategy: "steps"
  eval_steps: 100
  save_strategy: "steps"
  save_steps: 200
  save_total_limit: 10
  learning_rate: 2e-5
  weight_decay: 0.
  warmup_ratio: 0.01
  lr_scheduler_type: "cosine"
  logging_steps: 1
  fsdp: "full_shard auto_wrap"
  fsdp_transformer_layer_cls_to_wrap: 'LlamaDecoderLayer'
  tf32: True
  model_max_length: 2048
  gradient_checkpointing: True
  lazy_preprocess: True
